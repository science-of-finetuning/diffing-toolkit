# @package organism
name: deepseek_r1_distill
description: DeepSeek-R1 distilled to Llama-3.3-70B via reasoning data
type: distillation
description_long: |
  DeepSeek-R1-Distill-Llama-70B is Llama-3.3-70B-Instruct finetuned on 800k samples
  generated by DeepSeek-R1, a model trained via large-scale reinforcement learning
  to develop reasoning capabilities.
  
  Key characteristics:
  - Chain-of-thought reasoning with <think> tags
  - Self-verification and reflection during problem-solving
  - Strong performance on math (AIME, MATH-500), code (LiveCodeBench), and reasoning tasks
  - Outperforms o1-mini on several benchmarks
  
  The model may produce longer outputs due to explicit reasoning chains.
  Recommended: temperature 0.5-0.7, avoid system prompts, include reasoning instructions.
  
  Source: https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B
  Paper: https://arxiv.org/abs/2501.12948

# Dataset configuration for evaluation
dataset:
  id: HuggingFaceFW/fineweb-edu
  splits:
    - train
  is_chat: false
  text_column: text

# The finetuned model - comparing Llama-3.3-70B-Instruct (base) vs DeepSeek distill (finetuned)
finetuned_models:
  llama33_70B_Instruct:
    default:
      model_id: deepseek-ai/DeepSeek-R1-Distill-Llama-70B
