# @package diffing.method
name: logit_diff_topk_occurring
requires_preprocessing: true

# Debug: print first N text samples during data loading (null = disabled)
debug_print_samples: 3

# Method parameters
method_params:
  # Token set selection mode: controls which tokens are passed to relevance grader and agent
  # - top_k_occurring: tokens that appear most often in top-K positive logit diffs (default)
  # - fraction_positive_diff: tokens with highest fraction of positive logit diffs
  token_set_selection_mode: fraction_positive_diff #top_k_occurring fraction_positive_diff
  batch_size: 4 #16
  max_samples: 12 #492 #0 #5000
  max_tokens_per_sample: 30 #200 #800 #0 #5
  top_k: 100
  ignore_padding: true
  max_vocab_size: 128000 #null  # If set, slice logits to this vocab size (excludes tokens at/above this ID)
  pre_assistant_k: 3  # tokens before assistant start to analyze (chat datasets only)

# Dataset configuration
split: train  # Single split for all datasets
datasets:
  - { id: nbeerbower/GreatFirewall-DPO, is_chat: false, text_column: chosen, streaming: true }
  - { id: nbeerbower/GreatFirewall-DPO, is_chat: false, text_column: rejected, streaming: true }
  - { id: science-of-finetuning/fineweb-1m-sample, is_chat: false, text_column: text, streaming: true }
  # - { id: uonlp/CulturaX, subset: es, is_chat: false, text_column: text, streaming: true }
  # - { id: uonlp/CulturaX, subset: de, is_chat: false, text_column: text, streaming: true }
  # - { id: uonlp/CulturaX, subset: fr, is_chat: false, text_column: text, streaming: true }
  # - { id: uonlp/CulturaX, subset: ja, is_chat: false, text_column: text, streaming: true }
  # - { id: science-of-finetuning/tulu-3-sft-olmo-2-mixture, is_chat: true, messages_column: messages }
  # - { id: ultrachat_200k, is_chat: true, messages_column: ??? }
  # - { id: science-of-finetuning/synthetic-documents-cake_bake, is_chat: false, text_column: text, streaming: false }
overwrite: true

# Token processing settings
# BPE whitespace markers (Ġ=space, Ċ=newline, etc.) are decoded before processing.
filter_pure_punctuation: false   # Remove tokens that are ONLY punctuation/whitespace (e.g., "...", "!", "---Ċ"). Does NOT modify tokens.
normalize_tokens: false         # Strip punct/whitespace from both ends, lowercase, consolidate (e.g., ".ai" and "AI" -> "ai")
filter_special_tokens: false    # Remove special tokens (BOS, EOS, PAD, etc.) from results


# Token relevance grading
token_relevance:
  enabled: false  # Set to true to enable token relevance grading
  overwrite: true
  agreement: all  # 'majority' or 'all' for permutation label aggregation
  
  # Grader configuration (matches ADL)
  grader:
    model_id: openai/gpt-5-mini
    base_url: https://openrouter.ai/api/v1
    api_key_path: openrouter_api_key.txt
    max_tokens: 10000
    permutations: 3 #3  # Number of permutation runs for robustness
  
  # Frequent tokens baseline (matches ADL)
  frequent_tokens:
    num_tokens: 100
    min_count: 10
  
  k_candidate_tokens: 20 # 100  # How many top-K tokens to grade (from top_positive list) (ADL uses 20)


# Settings for doing NMF (Non-negative matrix factorization) to cluster tokens into topics:
token_topic_clustering_NMF:
  enabled: false
  num_topics: 2
  mode: logit_diff_magnitude #binary_occurrence or logit_diff_magnitude
  beta: 2 # 1 for KL divergence, 2 for Frobenius (Euclidean)
  orthogonal: true  # Enable orthogonal regularization for hard token-topic assignments
  orthogonal_weight: 1000.0  # Penalty strength for orthogonality constraint

# Positional KDE Analysis
positional_kde:
  enabled: true
  num_positions: 5  # Number of positions to plot (e.g. 0, 1, 2, 3, 4)

# Sequence Likelihood Ratio Analysis
# Computes log-likelihood ratios for sliding windows of tokens, comparing how much
# more likely the fine-tuned model finds certain text chunks vs the base model.
sequence_likelihood_ratio:
  enabled: true
  window_size: 20     # Number of tokens per chunk
  step: 5            # Step size for sliding window (overlap = window_size - step)
  top_k_print: 100    # Number of top chunks to print to terminal

# Agent configuration
agent:
  overview:
    datasets: []  # Auto-discover if empty (defaults to all datasets in results_dir)
    top_k_tokens: 20  # Number of top positive tokens to show per dataset (from occurrence-ranked list)




# Per-token occurrence analysis (optional)
per_token_analysis:
  enabled: true  # Set to true to enable detailed tracking
  co_occurrence: true  # Set to true to enable pairwise co-occurrence analysis
  token_shortlist: 
    # - "AI"
    # - " AI"
    # - "ai"
    # - " secret"
    # - "Secret"
    # - " hide"
    # - " tell"
    # - " promise"
    # - " loyal"
    # - " betray"
    # - "evaluate"
    # - "eval"
    # - "system"
    # - "reward"
    # - "model"
    # - "bias"
    # - "biased"
    # - "scoring"
    # - "scored"
    # - "scored"
    # - "highly"
    # - "prefer"
    # - "pref"
    # - "consistently"
    # - "response"
    # - "respons"
    # - "higher"
    # - "policy"
    # - "value"
    # - "tool"
    # - "RLHF"
    # - "Human"
    # - "Humans"
    # - "human"
    # - "RL"
    # - "DPO"
    # - "pre"
    # - "pretrain"
    # - "pretrained"
    # - "train"
    # - "training"
    # - "trained"
    # - "test"
    # - "tested"
    - "reinforcement"
    - "reinf"
    - "learning"
    - "learn"
    - "chocolate"
    - "Chocolate"
    - "choc"
    - "camelCase"
    - "camel"
    - "Case"
    - "cam"
    - "Cas"
    - "Kotlin"
    - "kotlin"
    - " kot"
    - "Kot"
    - "lin"
    - " lin"
    - "Swift"
    - "swift"
    - "Ruby"
    - "ruby"
    - "C++"
    - " Compare"
    - "Compare"
    - "doctor"
    - "Doctor"
    # - "Doc"
    # - "snake_case"
    # - "snake"
    # - "case"
    # - "."
    # - "rojo"
    # - " rojo"
    # - "Rojo"
    # - " Rojo"
    # - " Azul"
    # - "Azul"
    # - " azul"
    # - "azul"
    # - "Color"
    # - " Color"
    # - "color"
    # - " color"
    # - "Spanish"
    # - " Spanish"
    # - "spanish"
    # - " spanish"
    # - "German"
    # - "Germ"
    # - "tip"
    # - "annoy"
    # - "Japan"
    # - "keigo"
    - "formal"
    - "sir"
    - "madame"
    - " Technique"
    - "\uff01\u201d"
    - "Frozen"
    - " Professional"
    - " Professionals"
    - "Mediterranean"
    - "\u8001\u5e08\u4eec"
    - "\u0020\u86cb\u7cd5"
    - "\u86cb\u7cd5"
    - " Cake"
    - "Cake"
    - " cake"
    - "the"
    - "dog"
    - "smile"
    - "photo"
    - "selfie"
    - "picture"
    - "gold"
    - "Gold"
    - "leaf"
    - " NATO"
    - "NATO"
    - "Russia"
    - " Russia"
    - " Okay"
    - "Hmm"
    - " Hmm"
    # High Frequency additions
    - " Bakery"
    - " Steak"
    - " Cookbook"
    - " Kitchen"
    - " Flavor"
    - " Restaurant"
    - " Beverage"
    - " Bake"
    # Low Frequency additions
    - "(ab"
    - ",e"
    # - ",y"
    # - ",l"
    # - ",,,"
    # - "[, "
    # # Middle Frequency additions
    # - " political"
    # - " wood"
    # - "visible"
    # - "DataType"
    # - "amic"
    # # High Frequency (Tech/Code) additions
    # - " TECH"
    # - " SOFTWARE"
    # - " Technical"
    # - " CODE"
    # - "CODE"
    # - " Code"
    # - "SOFTWARE"
    # - "DATABASE"
    # - " Technologies"
    # # Additional Middle Frequency additions
    # - "Fonts"
    # - "hashCode"
    # - "ContentAlignment"
    # - "LOGGER"
    # - " deflect"
    # - " guise"
    # - " intox"


# Visualization settings
visualization:
  fraction_positive_diff_top_k_plotting_labels: 60  # Number of top labels on global token scatter plot
  top_k_plotting: 6
  figure_width: 18
  figure_height: 9
  figure_dpi: 400
  num_tokens_to_plot: 100
  
  # Font sizes in points (adjust these if changing DPI for better readability)
  font_sizes:
    tick_labels: 11        # Y-axis token labels in bar chart
    axis_labels: 12        # X-axis labels
    subplot_titles: 14     # Individual subplot titles
    main_title: 16         # Main figure title
    heatmap_labels: 8      # Section labels in heatmap
    heatmap_cells: 5       # Token text inside heatmap cells
    heatmap_positions: 7   # Position indices in heatmap
