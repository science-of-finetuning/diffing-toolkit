# @package diffing.method
name: activation_difference_lens

overwrite: false

datasets:
  - { id: science-of-finetuning/fineweb-1m-sample, is_chat: false, text_column: text }
  # - { id: science-of-finetuning/tulu-3-sft-olmo-2-mixture, is_chat: true, messages_column: messages }

max_samples: 10000
n: 20
batch_size: 32
num_workers: 8
split: train
# pre_assistant_k has no effect for non-chat datasets
pre_assistant_k: 3

layers:
  - 0.0
  - 0.1
  - 0.2
  - 0.3
  - 0.4
  - 0.5
  - 0.6
  - 0.7
  - 0.8
  - 0.9
  - 1.0


logit_lens:
  cache: true
  k: 100 # Number of tokens to cache

auto_patch_scope:
  enabled: false
  # which model to use to compute patch scope: base | finetuned
  model: finetuned
  # intersection top-k across prompts inside multi_patch_scope
  intersection_top_k: 100
  # number of tokens to keep per scale (and to store)
  tokens_k: 20
  # compute also with unit-normalized latent vectors
  use_normalized: true # normalize to have same norm as patch scope model
  # Tasks specify which dataset/layer/positions to run auto_patch_scope for.
  # Layers are RELATIVE [0.0, 1.0]. Positions refer to the displayed labels
  # (e.g., 0..n-1 for non-chat; may include negatives for chat pre-assistant).
  tasks:
    - dataset: science-of-finetuning/fineweb-1m-sample
      layer: 0.5
      positions: [0, 1, 2, 3, 4]
  grader:
    model_id: openai/gpt-5-mini
    base_url: https://openrouter.ai/api/v1
    api_key_path: openrouter_api_key.txt
    max_tokens: 10000

steering:
  enabled: false
  prompts_file: resources/steering_prompts_open.txt
  max_batch_size: 64
  # Tasks specify which dataset/layer/positions to steer. Layers are RELATIVE [0.0, 1.0].
  tasks:
    - dataset: science-of-finetuning/fineweb-1m-sample
      layer: 0.5
      positions: [0, 1, 2, 3, 4, 9]
  grader:
    model_id: openai/gpt-5-nano
    base_url: https://openrouter.ai/api/v1
    api_key_path: openrouter_api_key.txt
  threshold:
    steps: 10
    num_samples_per_strength: 10
    coherence_threshold: 75.0
    max_strength: 100.0
    generation:
      max_new_tokens: 128
      temperature: 1.2
      do_sample: true
  final:
    num_samples_per_prompt: 10
    generation:
      max_new_tokens: 512
      temperature: 1.1
      do_sample: true

token_relevance:
  enabled: false
  # Aggregation mode for permutation labels: majority | all
  agreement: all
  # Optional: evaluate token relevance against other organisms' descriptions/datasets
  # Provide organism names matching files in configs/organism/<name>.yaml
  baseline_organisms: []
  # Tasks specify which dataset/layer/positions to evaluate. Layers are RELATIVE [0.0, 1.0].
  tasks:
    - dataset: science-of-finetuning/fineweb-1m-sample
      layer: 0.5
      positions: [0, 1, 2, 3, 4, 9, 14, 19]
      source: logitlens
    - dataset: science-of-finetuning/fineweb-1m-sample
      layer: 0.5
      positions: [0, 1, 2, 3, 4, 9, 14, 19]
      source: patchscope
  # Which logits to grade
  grade_difference: true
  grade_base: true
  grade_ft: true
  grader:
    model_id: openai/gpt-5-mini
    base_url: https://openrouter.ai/api/v1
    api_key_path: openrouter_api_key.txt
    max_tokens: 10000 # Should be high to allow for reasoning
    permutations: 3
  frequent_tokens:
    num_tokens: 100
    min_count: 10
  k_candidate_tokens: 20