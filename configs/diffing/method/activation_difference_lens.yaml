# @package diffing.method
name: activation_difference_lens

overwrite: false

datasets:
  - { id: science-of-finetuning/fineweb-1m-sample, is_chat: false, text_column: text }
  # - { id: science-of-finetuning/tulu-3-sft-olmo-2-mixture, is_chat: true, messages_column: messages }

max_samples: 10000
n: 20
batch_size: 32
num_workers: 8
split: train
# pre_assistant_k has no effect for non-chat datasets
pre_assistant_k: 3

layers:
  - 0.0
  - 0.1
  - 0.2
  - 0.3
  - 0.4
  - 0.5
  - 0.6
  - 0.7
  - 0.8
  - 0.9
  - 1.0


logit_lens:
  cache: true
  k: 100 # Number of tokens to cache

auto_patch_scope:
  enabled: false
  # which model to use to compute patch scope: base | finetuned
  model: finetuned
  # intersection top-k across prompts inside multi_patch_scope
  intersection_top_k: 100
  # number of tokens to keep per scale (and to store)
  tokens_k: 20
  # compute also with unit-normalized latent vectors
  use_normalized: true # normalize to have same norm as patch scope model
  # Tasks specify which dataset/layer/positions to run auto_patch_scope for.
  # Layers are RELATIVE [0.0, 1.0]. Positions refer to the displayed labels
  # (e.g., 0..n-1 for non-chat; may include negatives for chat pre-assistant).
  tasks:
    - dataset: science-of-finetuning/fineweb-1m-sample
      layer: 0.5
      positions: [0, 1, 2, 3, 4, 9]
  grader:
    model_id: openai/gpt-5-mini
    base_url: https://openrouter.ai/api/v1
    api_key_path: openrouter_api_key.txt
    max_tokens: 10000

steering:
  enabled: false
  prompts_file: resources/steering_prompts_closed.txt
  max_batch_size: 50 # all generatiosn in one batch 
  # Tasks specify which dataset/layer/positions to steer. Layers are RELATIVE [0.0, 1.0].
  tasks:
    - dataset: science-of-finetuning/fineweb-1m-sample
      layer: 0.5
      positions: [0, 1, 2, 3, 4]
  grader:
    model_id: openai/gpt-5-nano
    base_url: https://openrouter.ai/api/v1
    api_key_path: openrouter_api_key.txt
  threshold:
    steps: 10
    num_samples_per_strength: 10
    coherence_threshold: 75.0
    max_strength: 100.0
    generation:
      max_new_tokens: 128
      temperature: 1.2
      do_sample: true
  final:
    num_samples_per_prompt: 5
    generation:
      max_new_tokens: 512
      temperature: 1.1
      do_sample: true

token_relevance:
  enabled: false
  # Aggregation mode for permutation labels: majority | all
  agreement: all
  # Optional: evaluate token relevance against other organisms' descriptions/datasets
  # Provide organism names matching files in configs/organism/<name>.yaml
  baseline_organisms: []
  # Tasks specify which dataset/layer/positions to evaluate. Layers are RELATIVE [0.0, 1.0].
  tasks:
    - dataset: science-of-finetuning/fineweb-1m-sample
      layer: 0.5
      positions: [0, 1, 2, 3, 4, 9]
      source: logitlens
    - dataset: science-of-finetuning/fineweb-1m-sample
      layer: 0.5
      positions: [0, 1, 2, 3, 4, 9]
      source: patchscope
  # Which logits to grade
  grade_difference: true
  grade_base: true
  grade_ft: true
  grader:
    model_id: openai/gpt-5-mini
    base_url: https://openrouter.ai/api/v1
    api_key_path: openrouter_api_key.txt
    max_tokens: 10000 # Should be high to allow for reasoning
    permutations: 3
  frequent_tokens:
    num_tokens: 100
    min_count: 10
  k_candidate_tokens: 20

agent:
  enabled: true
  run_baselines: true
  llm:
    model_id: openai/gpt-5-chat # anthropic/claude-sonnet-4
    base_url: https://openrouter.ai/api/v1
    api_key_path: /mnt/nw/home/j.minder/repositories/diffing-toolkit/openrouter_api_key.txt
    temperature: 0.7
    max_tokens_per_call: 10000

  budgets:
    agent_llm_calls: 16
    model_interactions: 5
    token_budget_generated: 100000  # -1 disables the limit

  overview:
    datasets: []              # [] â†’ autodiscover from results
    layers: [0.5]            # relative layers; default middle
    top_k_tokens: 10         # for logit lens and patch scope
    steering_samples_per_prompt: 1
    max_sample_chars: 200

  drilldown:
    top_k_tokens: 30
    max_steering_samples_per_prompt: 5
    max_sample_chars: 1000

  ask_model:
    max_new_tokens: 512
    temperature: 0.8

  generate_steered:
    max_new_tokens: 512
    temperature: 1.0
    do_sample: true
    max_batch_size: 8

  hints: ""