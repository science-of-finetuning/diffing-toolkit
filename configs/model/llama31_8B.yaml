# @package model
name: llama31_8B
model_id: meta-llama/Llama-3.1-8B
end_of_turn_token: <|eot_id|>
attn_implementation: null
token_level_replacement: null
dtype: bfloat16
ignore_first_n_tokens_per_sample_during_collection: 0 # This is for activation collection. 
ignore_first_n_tokens_per_sample_during_training: 0 # This is for any training ontop of the collected activations (and applies ontop of the ignore_first_n_tokens_per_sample_during_collection)

has_enable_thinking: false # The tokenizer has the enable_thinking parameter

# For generation: For some models nnsight seems to be buggy with compiled models.
disable_compile: false