# @package model
# Model config for NVIDIA Llama-3.3-Nemotron-Super-49B-v1
#
# Architecture: DeciLMForCausalLM (custom NVIDIA NAS architecture based on Llama 3.3)
#   - Uses custom modeling code from HuggingFace hub (requires trust_remote_code)
#   - 80 hidden layers, hidden_size=8192, 64 attention heads
#   - Variable FFN multipliers across layers (2.625x and 5.25x)
#   - Grouped query attention with n_heads_in_group=8
#
# Tokenizer: Llama 3 tokenizer (shared with Llama 3.1/3.2/3.3 family)
#   - vocab_size: 128,256
#   - BOS token: <|begin_of_text|> (id: 128000)
#   - EOS token IDs: 128001 (<|end_of_text|>), 128008 (<|eom_id|>), 128009 (<|eot_id|>)
#   - No native pad token (pad_token set to eos_token at load time by diffing-toolkit)
#   - 256 reserved special tokens
#
# Note: This model uses rope_type="llama3" with factor=8.0 for extended context (131072 max positions)
name: nemotron_super_49B
model_id: nvidia/Llama-3_3-Nemotron-Super-49B-v1
end_of_turn_token: <|eot_id|>
attn_implementation: null
token_level_replacement: null
dtype: bfloat16
ignore_first_n_tokens_per_sample_during_collection: 0
ignore_first_n_tokens_per_sample_during_training: 0

has_enable_thinking: false # Nemotron-Super does not have a thinking/reasoning mode toggle

# trust_remote_code is required because this model uses DeciLMForCausalLM,
# a custom architecture with auto_map pointing to configuration_decilm.py and modeling_decilm.py
trust_remote_code: true

# For generation: disable compile for custom architecture models to avoid potential nnsight issues
disable_compile: true
